{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT Hyperparameter Tuning\n",
    "\n",
    "This notebook runs through hyperparameter tuning for the SUPPORT dataset. For this we use the Optuna library.\n",
    "\n",
    "For users with limited computational power OR with no access to MIMIC datasets.\n",
    "\n",
    "We validate our hyperparameter tuning results on our training dataset metrics - This is isn't optimal as usually it would be validated on a separate validation set. Hard to create an appropriate validation set in this instance as we would require the distributions for each variable column to look similar between training & validation.\n",
    "\n",
    "NOTE: There are known limitations that are explained as they come up in these markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# VAE is in other folder as well as opacus adapted library\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Opacus support for differential privacy\n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "\n",
    "# For the SUPPORT dataset\n",
    "from pycox.datasets import support\n",
    "\n",
    "# For VAE dataset formatting\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# VAE functions\n",
    "from VAE import Decoder, Encoder, VAE\n",
    "\n",
    "# Utility file contains all functions required to run notebook\n",
    "from utils import (\n",
    "    support_pre_proc,\n",
    "    plot_elbo,\n",
    "    plot_likelihood_breakdown,\n",
    "    plot_variable_distributions,\n",
    "    reverse_transformers,\n",
    ")\n",
    "from metrics import distribution_metrics\n",
    "\n",
    "# For hyperparameter tuning as well as saving different trial objects\n",
    "import optuna\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Column Definitions\n",
    "\n",
    "First we load in the SUPPORT dataset from pycox datasets. Then we define the continuous and categorical columns in that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the support data\n",
    "data_supp = support.read_df()\n",
    "\n",
    "# Save the original columns\n",
    "\n",
    "original_continuous_columns = [\"duration\"] + [f\"x{i}\" for i in range(7, 15)]\n",
    "original_categorical_columns = [\"event\"] + [f\"x{i}\" for i in range(1, 7)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "Data can be pre-processed in 2 ways. Either we use <b>\"standard\"</b> option which performs a standard scaler on continuous variables - This has known limitations as:\n",
    "\n",
    "- Data in tables is usually non-gaussian and SynthVAE implements a gaussian loss, so this will perform worse unless the data is KNOWN to follow a gaussian distribution already.\n",
    "\n",
    "Or we use the second option of <b>\"GMM\"</b>. This performs a variational gaussian mixture model to scale the data & transform it to a gaussian distribution. We use a maximum number of clusters of 10 but the variational method will select the best number of clusters for that continuous variable. This also has known limitations:\n",
    "\n",
    "- 10 Clusters is arbitrary and may not be enough for certain variables.\n",
    "- We are fitting a model to transform the data and hence we are approximating before model is trained. This will lose fidelity as the distribution will not be transformed perfectly.\n",
    "\n",
    "SUPPORT is a limited dataset as it has no missingness (which our model currently does NOT handle) and it has no datetime columns or other data types. Be wary drawing any conclusions from this set due to these constraints as well as the dataset size. Testing/training new models with this set could be useful but conclusive results should be tested on other sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dxb085\\Anaconda3\\envs\\SynthVAE\\lib\\site-packages\\pandas\\core\\frame.py:3069: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\dxb085\\Anaconda3\\envs\\SynthVAE\\lib\\site-packages\\sklearn\\mixture\\_base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn('Initialization %d did not converge. '\n",
      "C:\\Users\\dxb085\\Anaconda3\\envs\\SynthVAE\\lib\\site-packages\\sklearn\\mixture\\_base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn('Initialization %d did not converge. '\n",
      "C:\\Users\\dxb085\\Anaconda3\\envs\\SynthVAE\\lib\\site-packages\\sklearn\\mixture\\_base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn('Initialization %d did not converge. '\n",
      "C:\\Users\\dxb085\\Anaconda3\\envs\\SynthVAE\\lib\\site-packages\\sklearn\\mixture\\_base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn('Initialization %d did not converge. '\n",
      "C:\\Users\\dxb085\\Anaconda3\\envs\\SynthVAE\\lib\\site-packages\\sklearn\\mixture\\_base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn('Initialization %d did not converge. '\n",
      "C:\\Users\\dxb085\\Anaconda3\\envs\\SynthVAE\\lib\\site-packages\\sklearn\\mixture\\_base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn('Initialization %d did not converge. '\n",
      "C:\\Users\\dxb085\\Anaconda3\\envs\\SynthVAE\\lib\\site-packages\\sklearn\\mixture\\_base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn('Initialization %d did not converge. '\n",
      "C:\\Users\\dxb085\\Anaconda3\\envs\\SynthVAE\\lib\\site-packages\\sklearn\\mixture\\_base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn('Initialization %d did not converge. '\n",
      "C:\\Users\\dxb085\\Anaconda3\\envs\\SynthVAE\\lib\\site-packages\\sklearn\\mixture\\_base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn('Initialization %d did not converge. '\n"
     ]
    }
   ],
   "source": [
    "#%% -------- Data Pre-Processing -------- #\n",
    "\n",
    "pre_proc_method = \"GMM\"\n",
    "\n",
    "(\n",
    "    x_train,\n",
    "    data_supp,\n",
    "    reordered_dataframe_columns,\n",
    "    continuous_transformers,\n",
    "    categorical_transformers,\n",
    "    num_categories,\n",
    "    num_continuous,\n",
    ") = support_pre_proc(data_supp=data_supp, pre_proc_method=pre_proc_method)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation & Training of VAE.\n",
    "\n",
    "We can adapt certain parameters of the model e.g. batch size, latent dimension size etc. This model implements early stopping and these values can be adapted.\n",
    "\n",
    "We can also activate differential privacy by implementing dp-sgd through the opacus library.\n",
    "\n",
    "The user defined parameters are defined first and these are arbitrary. For example you could change batch size as well as other variables and if you wanted to do this then you simply move batch size into the objective function in the cell below and then follow the Optuna guidelines on creating a hyperparameter selection.\n",
    "\n",
    "NOTE: training can be fail and cause errors if the hyperparameter values are not chosen carefully. In this example learning rate was left as <i>1e-3</i> rather than adapted as giving it a selection lead to errors in the training of the encoder - something to watch out for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Create & Train VAE -------- #\n",
    "\n",
    "# User defined parameters\n",
    "\n",
    "# General training\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "logging_freq = 1  # Number of epochs we should log the results to the user\n",
    "patience = 5  # How many epochs should we allow the model train to see if\n",
    "# improvement is made\n",
    "delta = 10  # The difference between elbo values that registers an improvement\n",
    "filepath = None  # Where to save the best model\n",
    "\n",
    "\n",
    "# Privacy params\n",
    "differential_privacy = False  # Do we want to implement differential privacy\n",
    "sample_rate = 0.1  # Sampling rate\n",
    "noise_scale = None  # Noise multiplier - influences how much noise to add\n",
    "target_eps = 1  # Target epsilon for privacy accountant\n",
    "target_delta = 1e-5  # Target delta for privacy accountant\n",
    "\n",
    "# Define the metrics you want the model to evaluate\n",
    "\n",
    "gower = False\n",
    "\n",
    "# Prepare data for interaction with torch VAE\n",
    "Y = torch.Tensor(x_train)\n",
    "dataset = TensorDataset(Y)\n",
    "\n",
    "generator = None\n",
    "sample_rate = batch_size / len(dataset)\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_sampler=UniformWithReplacementSampler(\n",
    "        num_samples=len(dataset), sample_rate=sample_rate, generator=generator\n",
    "    ),\n",
    "    pin_memory=True,\n",
    "    generator=generator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Optuna Hyperparameter Tuning Objective Function\n",
    "\n",
    "See markdown above for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Define our Optuna trial -------- #\n",
    "\n",
    "\n",
    "def objective(\n",
    "    trial,\n",
    "    gower,\n",
    "    differential_privacy=False,\n",
    "    target_delta=1e-3,\n",
    "    target_eps=10.0,\n",
    "    n_epochs=50,\n",
    "):\n",
    "\n",
    "    latent_dim = trial.suggest_int(\"Latent Dimension\", 2, 128, step=2)  # Hyperparam\n",
    "    hidden_dim = trial.suggest_int(\"Hidden Dimension\", 32, 1024, step=32)  # Hyperparam\n",
    "\n",
    "    encoder = Encoder(x_train.shape[1], latent_dim, hidden_dim=hidden_dim)\n",
    "    decoder = Decoder(latent_dim, num_continuous, num_categories=num_categories)\n",
    "\n",
    "    lr = trial.suggest_float(\"Learning Rate\", 1e-3, 1e-2, step=1e-5)\n",
    "    vae = VAE(encoder, decoder, lr=1e-3)  # lr hyperparam\n",
    "\n",
    "    C = trial.suggest_int(\"C\", 10, 1e4, step=50)  # Clipping hyperparam\n",
    "\n",
    "    if differential_privacy == True:\n",
    "        (\n",
    "            training_epochs,\n",
    "            log_elbo,\n",
    "            log_reconstruction,\n",
    "            log_divergence,\n",
    "            log_categorical,\n",
    "            log_numerical,\n",
    "        ) = vae.diff_priv_train(\n",
    "            data_loader,\n",
    "            n_epochs=n_epochs,\n",
    "            C=C,  # Hyperparam\n",
    "            target_eps=target_eps,\n",
    "            target_delta=target_delta,\n",
    "            sample_rate=sample_rate,\n",
    "        )\n",
    "        print(f\"(epsilon, delta): {vae.get_privacy_spent(target_delta)}\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        (\n",
    "            training_epochs,\n",
    "            log_elbo,\n",
    "            log_reconstruction,\n",
    "            log_divergence,\n",
    "            log_categorical,\n",
    "            log_numerical,\n",
    "        ) = vae.train(data_loader, n_epochs=n_epochs)\n",
    "\n",
    "    # -------- Synthetic Data Generation -------- #\n",
    "\n",
    "    synthetic_sample = vae.generate(data_supp.shape[0])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        synthetic_sample = pd.DataFrame(\n",
    "            synthetic_sample.cpu().detach(), columns=reordered_dataframe_columns\n",
    "        )\n",
    "    else:\n",
    "        synthetic_sample = pd.DataFrame(\n",
    "            synthetic_sample.detach(), columns=reordered_dataframe_columns\n",
    "        )\n",
    "\n",
    "    # Reverse the transformations\n",
    "\n",
    "    synthetic_supp = reverse_transformers(\n",
    "        synthetic_set=synthetic_sample,\n",
    "        data_supp_columns=data_supp.columns,\n",
    "        cont_transformers=continuous_transformers,\n",
    "        cat_transformers=categorical_transformers,\n",
    "        pre_proc_method=pre_proc_method,\n",
    "    )\n",
    "    # -------- SDV Metrics -------- #\n",
    "\n",
    "    metrics = distribution_metrics(\n",
    "        gower_bool=gower,\n",
    "        data_supp=data_supp,\n",
    "        synthetic_supp=synthetic_supp,\n",
    "        categorical_columns=original_categorical_columns,\n",
    "        continuous_columns=original_continuous_columns,\n",
    "        saving_filepath=None,\n",
    "        pre_proc_method=pre_proc_method,\n",
    "    )\n",
    "\n",
    "    # Optuna wants a list of values in float form\n",
    "\n",
    "    list_metrics = [metrics[i] for i in metrics.columns]\n",
    "\n",
    "    print(list_metrics)\n",
    "\n",
    "    return list_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Trials\n",
    "\n",
    "Here we use optuna to set up a study and run it for a predefined number of trials. If the study has not already been created then set <b>first_run</b> to True. This will then create the study for running.\n",
    "\n",
    "NOTE: directions show if we are maximising or minimising the metrics we are inputting. Most of SDV metrics require maximizing and that is why <b>directions</b> is set up like this. If you are inputting metrics that require minimizing then you need to set up your directions list accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no study object in your folder then run and save the study so\n",
    "# It can be resumed if needed\n",
    "\n",
    "# User parameters\n",
    "\n",
    "first_run = True  # First run indicates if we are creating a new hyperparam study\n",
    "saving_filepath = \"\"  # To save the study if you wish - needs to be .pkl format\n",
    "n_trials = 3  # Number of trials you want to hyperparameter tune for\n",
    "loading_filepath = None  # For loading any older study objects - needs to be .pkl format\n",
    "\n",
    "if first_run == True:\n",
    "\n",
    "    if gower == True:\n",
    "\n",
    "        directions = [\"maximize\" for i in range(8)]\n",
    "\n",
    "    else:\n",
    "\n",
    "        directions = [\"maximize\" for i in range(7)]\n",
    "\n",
    "    study = optuna.create_study(directions=directions)\n",
    "\n",
    "else:\n",
    "\n",
    "    with open(\"{}\".format(loading_filepath), \"rb\") as f:\n",
    "        study = pickle.load(f)\n",
    "\n",
    "study.optimize(\n",
    "    lambda trial: objective(\n",
    "        trial,\n",
    "        gower=gower,\n",
    "        differential_privacy=differential_privacy,\n",
    "        target_delta=target_delta,\n",
    "        target_eps=target_eps,\n",
    "        n_epochs=n_epochs,\n",
    "    ),\n",
    "    n_trials=n_trials,\n",
    "    gc_after_trial=True,\n",
    ")  # GC to avoid OOM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving The Study\n",
    "\n",
    "Here we use pickle to save the study so that it can be loaded up and run from its current point.\n",
    "\n",
    "If your study is a multi objective study then it will give you multiple <b>best_trials</b> when you use <b>study.best_trials</b>. Depending your weighting for each metric, you can decide which study you will pick as optimal. An example of this is shown in the second cell where each metric is equally important and we average over them using the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Save The  Study -------- #\n",
    "\n",
    "with open(\"{}\".format(saving_filepath), \"wb\") as f:\n",
    "    pickle.dump(study, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_averages = []\n",
    "\n",
    "for trials in study.best_trials:\n",
    "\n",
    "    metrics = trials.values\n",
    "    trial_averages.append(np.mean(metrics))\n",
    "\n",
    "# Now find the best trial\n",
    "\n",
    "best_trial = np.argmax(np.asarray(trial_averages))\n",
    "\n",
    "# Best trial hyperparameters\n",
    "\n",
    "study.best_trials[best_trial].params\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3faeeb7a141a1b6863ef3d83f2d4891432bfa1117b17a94d8e78eaa2bfb2ea7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('SynthVAE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
