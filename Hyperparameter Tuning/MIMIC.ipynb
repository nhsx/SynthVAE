{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC Hyperparameter Tuning\n",
    "\n",
    "This notebook runs through hyperparameter tuning for the internal MIMIC dataset. For this we use the Optuna library.\n",
    "\n",
    "For users who have access to the MIMIC-III internal datasets - currently the notebook to generate the internal datasets is not uploaded. However, if you create a similar dataset to the one provided within the data guidance file using the MIMIC-III dataset then you will be able to run through this.\n",
    "\n",
    "We validate our hyperparameter tuning results on our training dataset metrics - This is isn't optimal as usually it would be validated on a separate validation set. Hard to create an appropriate validation set in this instance as we would require the distributions for each variable column to look similar between training & validation.\n",
    "\n",
    "NOTE: There are known limitations that are explained as they come up in these markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "from tokenize import String\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# VAE is in other folder\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# Opacus support for differential privacy\n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "\n",
    "# For VAE dataset formatting\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# VAE functions\n",
    "from VAE import Decoder, Encoder, VAE\n",
    "\n",
    "# For datetime columns we need a transformer\n",
    "from rdt.transformers import datetime\n",
    "\n",
    "# Utility file contains all functions required to run notebook\n",
    "from utils import mimic_pre_proc, constraint_filtering, plot_elbo, plot_likelihood_breakdown, plot_variable_distributions, reverse_transformers\n",
    "from metrics import distribution_metrics\n",
    "\n",
    "# Hyperparameter tuning library as well as pickle to save study objects\n",
    "import optuna\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Column Definitions\n",
    "\n",
    "First we need to load in the MIMIC dataset from a specified filepath. \n",
    "\n",
    "We then need to create lists indicating which columns are:\n",
    "a) continuous\n",
    "b) categorical\n",
    "c) datetime\n",
    "\n",
    "Currently other data types are not supported. Importantly if columns contain missing data then they need to be dropped - Do not include these in original column lists & instead drop them from the loaded set in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the mimic single table data \n",
    "\n",
    "filepath = \"\"\n",
    "\n",
    "data_supp = pd.read_csv(filepath)\n",
    "# Save the original columns\n",
    "\n",
    "original_categorical_columns = ['Categorical_1', 'Categorical_2', ...]\n",
    "original_continuous_columns = ['Continuous_2', 'Continuous_2', ...]\n",
    "original_datetime_columns = ['Datetime_1', 'Datetime_2', ...]\n",
    "original_columns = original_categorical_columns + original_continuous_columns + original_datetime_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that have missing data as these cannot be handled in the current implementation\n",
    "\n",
    "data_supp = data_supp.drop('Missing_Column_1', axis = 1) # etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "Data can be pre-processed in 2 ways. Either we use <b>\"standard\"</b> option which performs a standard scaler on continuous variables - This has known limitations as:\n",
    "\n",
    "- Data in tables is usually non-gaussian and SynthVAE implements a gaussian loss, so this will perform worse unless the data is KNOWN to follow a gaussian distribution already.\n",
    "\n",
    "Or we use the second option of <b>\"GMM\"</b>. This performs a variational gaussian mixture model to scale the data & transform it to a gaussian distribution. We use a maximum number of clusters of 10 but the variational method will select the best number of clusters for that continuous variable. This also has known limitations:\n",
    "\n",
    "- 10 Clusters is arbitrary and may not be enough for certain variables.\n",
    "- We are fitting a model to transform the data and hence we are approximating before model is trained. This will lose fidelity as the distribution will not be transformed perfectly.\n",
    "\n",
    "\n",
    "For datasets that include datetime columns, original_metric_set returns the initial dataset after these columns have been transformed. This is because:\n",
    "\n",
    "- Our evaluation suite cannot calculate certain metrics on datetime objects so these need to be converted to continuous values first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_proc_method = \"GMM\" # Select pre-processing method standard or GMM\n",
    "\n",
    "#%% -------- Data Pre-Processing -------- #\n",
    "\n",
    "x_train, original_metric_set, reordered_dataframe_columns, continuous_transformers, categorical_transformers, datetime_transformers, num_categories, num_continuous = mimic_pre_proc(data_supp=data_supp, pre_proc_method=pre_proc_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation & Training of VAE.\n",
    "\n",
    "We can adapt certain parameters of the model e.g. batch size, latent dimension size etc. This model implements early stopping and these values can be adapted.\n",
    "\n",
    "We can also activate differential privacy by implementing dp-sgd through the opacus library.\n",
    "\n",
    "The user defined parameters are defined first and these are arbitrary. For example you could change batch size as well as other variables and if you wanted to do this then you simply move batch size into the objective function in the cell below and then follow the Optuna guidelines on creating a hyperparameter selection.\n",
    "\n",
    "NOTE: training can be fail and cause errors if the hyperparameter values are not chosen carefully. In this example learning rate was left as <i>1e-3</i> rather than adapted as giving it a selection lead to errors in the training of the encoder - something to watch out for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined parameters\n",
    "\n",
    "# General training\n",
    "batch_size=32\n",
    "n_epochs=5\n",
    "logging_freq=1 # Number of epochs we should log the results to the user\n",
    "patience=5 # How many epochs should we allow the model train to see if\n",
    "# improvement is made\n",
    "delta=10 # The difference between elbo values that registers an improvement\n",
    "filepath=None # Where to save the best model\n",
    "\n",
    "\n",
    "# Privacy params\n",
    "differential_privacy = False # Do we want to implement differential privacy\n",
    "sample_rate=0.1 # Sampling rate\n",
    "noise_scale=None # Noise multiplier - influences how much noise to add\n",
    "target_eps=1 # Target epsilon for privacy accountant\n",
    "target_delta=1e-5 # Target delta for privacy accountant\n",
    "\n",
    "# Define the metrics you want the model to evaluate\n",
    "\n",
    "gower=False\n",
    "\n",
    "# Prepare data for interaction with torch VAE\n",
    "Y = torch.Tensor(x_train)\n",
    "dataset = TensorDataset(Y)\n",
    "\n",
    "generator = None\n",
    "sample_rate = batch_size / len(dataset)\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_sampler=UniformWithReplacementSampler(\n",
    "        num_samples=len(dataset), sample_rate=sample_rate, generator=generator\n",
    "    ),\n",
    "    pin_memory=True,\n",
    "    generator=generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Optuna Hyperparameter Tuning Objective Function\n",
    "\n",
    "See markdown above for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Define our Optuna trial -------- #\n",
    "\n",
    "def objective(trial, gower, differential_privacy=False, target_delta=1e-3, target_eps=10.0, n_epochs=50):\n",
    "\n",
    "    latent_dim = trial.suggest_int('Latent Dimension', 2, 128, step=2) # Hyperparam\n",
    "    hidden_dim = trial.suggest_int('Hidden Dimension', 32, 1024, step=32) # Hyperparam\n",
    "\n",
    "    encoder = Encoder(x_train.shape[1], latent_dim, hidden_dim=hidden_dim)\n",
    "    decoder = Decoder(\n",
    "        latent_dim, num_continuous, num_categories=num_categories\n",
    "    )\n",
    "\n",
    "    lr = trial.suggest_float('Learning Rate', 1e-3, 1e-2, step=1e-5)\n",
    "    vae = VAE(encoder, decoder, lr=1e-3) # lr hyperparam\n",
    "\n",
    "    C = trial.suggest_int('C', 10, 1e4, step=50)\n",
    "\n",
    "    if differential_privacy == True:\n",
    "        log_elbo, log_reconstruction, log_divergence, log_categorical, log_numerical = vae.diff_priv_train(\n",
    "            data_loader,\n",
    "            n_epochs=n_epochs,\n",
    "            C=C, # Hyperparam\n",
    "            target_eps=target_eps,\n",
    "            target_delta=target_delta, \n",
    "            sample_rate=sample_rate,\n",
    "        )\n",
    "        print(f\"(epsilon, delta): {vae.get_privacy_spent(target_delta)}\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        log_elbo, log_reconstruction, log_divergence, log_categorical, log_numerical = vae.train(data_loader, n_epochs=n_epochs)\n",
    "\n",
    "    # -------- Generate Synthetic Data -------- #\n",
    "\n",
    "    synthetic_supp = constraint_filtering(\n",
    "    n_rows=data_supp.shape[0], vae=vae, reordered_cols=reordered_dataframe_columns,\n",
    "    data_supp_columns=data_supp.columns, cont_transformers=continuous_transformers,\n",
    "    cat_transformers=categorical_transformers, date_transformers=datetime_transformers,\n",
    "    pre_proc_method=pre_proc_method\n",
    "    )\n",
    "\n",
    "    # -------- Datetime Handling -------- #\n",
    "\n",
    "    # If the dataset has datetimes then we need to re-convert these to a numerical\n",
    "    # Value representing seconds, this is so we can evaluate the metrics on them\n",
    "\n",
    "    metric_synthetic_supp = synthetic_supp.copy()\n",
    "\n",
    "    for index, column in enumerate(original_datetime_columns):\n",
    "\n",
    "        # Fit datetime transformer - converts to seconds\n",
    "        temp_datetime = datetime.DatetimeTransformer()\n",
    "        temp_datetime.fit(metric_synthetic_supp, columns = column)\n",
    "\n",
    "        metric_synthetic_supp = temp_datetime.transform(metric_synthetic_supp)\n",
    "\n",
    "    # -------- SDV Metrics -------- #\n",
    "    # Calculate the sdv metrics for SynthVAE\n",
    "\n",
    "    metrics = distribution_metrics(\n",
    "    gower=gower, data_supp=data_supp, synthetic_supp=synthetic_supp,\n",
    "    categorical_columns=original_categorical_columns, continuous_columns=original_continuous_columns,\n",
    "    saving_filepath=None, pre_proc_method=pre_proc_method\n",
    "    )\n",
    "\n",
    "    # Optuna wants a list of values in float form\n",
    "\n",
    "    list_metrics = [metrics[i] for i in metrics.columns]\n",
    "\n",
    "    print(list_metrics)\n",
    "\n",
    "    return list_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Trials\n",
    "\n",
    "Here we use optuna to set up a study and run it for a predefined number of trials. If the study has not already been created then set <b>first_run</b> to True. This will then create the study for running.\n",
    "\n",
    "NOTE: directions show if we are maximising or minimising the metrics we are inputting. Most of SDV metrics require maximizing and that is why <b>directions</b> is set up like this. If you are inputting metrics that require minimizing then you need to set up your directions list accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no study object in your folder then run and save the study so\n",
    "# It can be resumed if needed\n",
    "\n",
    "# User parameters\n",
    "\n",
    "first_run=True  # First run indicates if we are creating a new hyperparam study\n",
    "saving_filepath = \"test.pkl\" # To save the study if you wish - needs to be .pkl format\n",
    "n_trials = 3 # Number of trials you want to hyperparameter tune for - needs to be .pkl format\n",
    "loading_filepath=None # To load any older study if they have already been created\n",
    "\n",
    "if(first_run==True):\n",
    "\n",
    "    if(gower==True):\n",
    "        directions = ['maximize' for i in range(6)]\n",
    "    else:\n",
    "        directions = ['maximize' for i in range(5)]\n",
    "\n",
    "    study = optuna.create_study(directions=directions)\n",
    "\n",
    "else:\n",
    "\n",
    "    with open('{}'.format(loading_filepath), 'rb') as f:\n",
    "        study = pickle.load(f)\n",
    "\n",
    "study.optimize(\n",
    "    lambda trial : objective(\n",
    "    trial, gower=gower, differential_privacy=differential_privacy, target_delta=target_delta, target_eps=target_eps, n_epochs=n_epochs\n",
    "    ), n_trials=n_trials, gc_after_trial=True\n",
    "    ) # GC to avoid OOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving The Study\n",
    "\n",
    "Here we use pickle to save the study so that it can be loaded up and run from its current point.\n",
    "\n",
    "If your study is a multi objective study then it will give you multiple <b>best_trials</b> when you use <b>study.best_trials</b>. Depending your weighting for each metric, you can decide which study you will pick as optimal. An example of this is shown in the second cell where each metric is equally important and we average over them using the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Save The  Study -------- #\n",
    "\n",
    "with open(\"{}\".format(saving_filepath), 'wb') as f:\n",
    "        pickle.dump(study, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_averages = []\n",
    "\n",
    "for trials in study.best_trials:\n",
    "\n",
    "    metrics = trials.values\n",
    "    trial_averages.append(np.mean(metrics))\n",
    "\n",
    "# Now find the best trial\n",
    "\n",
    "best_trial = np.argmax(np.asarray(trial_averages))\n",
    "\n",
    "# Best trial hyperparameters\n",
    "\n",
    "study.best_trials[best_trial].params"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3faeeb7a141a1b6863ef3d83f2d4891432bfa1117b17a94d8e78eaa2bfb2ea7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('SynthVAE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
